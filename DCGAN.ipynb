{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Deep Convolutional GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading CIFAR 10 dataset\n",
    "\n",
    "image_height = 32\n",
    "image_width = 32\n",
    "color_channels = 3\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "cifar_path = './cifar-10-python/'\n",
    "\n",
    "train_data = np.array([])\n",
    "train_labels = np.array([])\n",
    "\n",
    "# Load all the data batches.\n",
    "for i in range(1,6):\n",
    "    data_batch = unpickle(cifar_path + 'data_batch_' + str(i))\n",
    "    train_data = np.append(train_data, data_batch[b'data'])\n",
    "    train_labels = np.append(train_labels, data_batch[b'labels'])\n",
    "    \n",
    "# TODO: Process Cifar data\n",
    " \n",
    "def process_data(data):\n",
    "    float_data = np.array(data, dtype=float) / 255.0\n",
    "     \n",
    "    reshaped_data = np.reshape(float_data, (-1, color_channels, image_height, image_width))\n",
    "    transposed_data = np.transpose(reshaped_data, [0, 2, 3, 1])\n",
    "     \n",
    "    return transposed_data\n",
    " \n",
    "train_data = process_data(train_data)\n",
    "\n",
    "# TODO: Find labels of only the class we want\n",
    "\n",
    "train_class = 8\n",
    "\n",
    "class_train_data = []\n",
    "\n",
    "indices = [i for i, x in enumerate(train_labels) if x == train_class]\n",
    "\n",
    "for i in indices:\n",
    "    class_train_data.append(train_data[i])\n",
    "    \n",
    "class_train_data = np.array(class_train_data)\n",
    "print(class_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving image samples\n",
    "def save_sample(images_array, filename, shape):\n",
    "    \n",
    "    img_width = images_array.shape[1]\n",
    "    img_height = images_array.shape[2]\n",
    "    \n",
    "    final_width = img_width * shape[0]\n",
    "    final_height = img_width * shape[1]\n",
    "    \n",
    "    color_channels = images_array.shape[3]\n",
    "    \n",
    "    final_arr = np.zeros((final_width, final_height, color_channels))\n",
    "    \n",
    "    for i in range(len(images_array)):\n",
    "        x = int(i % shape[0]) * img_width\n",
    "        y = int(i / shape[0]) * img_height\n",
    "        \n",
    "        final_arr[x:x + img_width, y:y + img_height] = images_array[i].reshape(img_height, img_width, color_channels)\n",
    "        \n",
    "    final_img = Image.fromarray((final_arr * 255).astype(np.uint8), mode=\"RGB\")\n",
    "    final_img.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Network\n",
    "\n",
    "def create_generator(z, reuse=False):\n",
    "    with tf.variable_scope(\"generator\", reuse=reuse) as scope:\n",
    "        dense = tf.layers.dense(z, 4096, kernel_initializer=initializer, bias_initializer=initializer)\n",
    "        norm = tf.contrib.layers.batch_norm(dense)\n",
    "        a = tf.nn.relu(norm)\n",
    "        conv_input = tf.reshape(a, [-1, 4, 4, 256])\n",
    "        \n",
    "        conv = tf.layers.conv2d_transpose(conv_input, 128, [5, 5], strides=2, padding=\"same\",\n",
    "                                          bias_initializer=initializer, kernel_initializer=initializer)\n",
    "        norm = tf.contrib.layers.batch_norm(conv)\n",
    "        a = tf.nn.relu(norm)\n",
    "        \n",
    "        conv = tf.layers.conv2d_transpose(a, 64, [5, 5], strides=2, padding=\"same\",\n",
    "                                          bias_initializer=initializer, kernel_initializer=initializer)\n",
    "        norm = tf.contrib.layers.batch_norm(conv)\n",
    "        a = tf.nn.relu(norm)\n",
    "        \n",
    "        conv = tf.layers.conv2d_transpose(a, color_channels, [5, 5], strides=2, padding=\"same\",\n",
    "                                          bias_initializer=initializer, kernel_initializer=initializer)\n",
    "        norm = tf.contrib.layers.batch_norm(conv)\n",
    "        a = tf.nn.relu(norm)\n",
    "        \n",
    "        conv = tf.layers.conv2d_transpose(a, color_channels, [7, 7], padding=\"same\",\n",
    "                                          bias_initializer=initializer, kernel_initializer=initializer)\n",
    "        output = tf.sigmoid(conv)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Network\n",
    "\n",
    "def create_discriminator(image, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse) as scope:\n",
    "        conv = tf.layers.conv2d(image, 64, [5, 5], padding=\"same\",\n",
    "                                bias_initializer=initializer, kernel_initializer=initializer)\n",
    "        norm = tf.contrib.layers.batch_norm(conv)\n",
    "        a = tf.nn.leaky_relu(norm)\n",
    "        pool = tf.layers.max_pooling2d(a, [2, 2], [2, 2], \"same\")\n",
    "        \n",
    "        conv = tf.layers.conv2d(pool, 128, [5, 5], padding=\"same\",\n",
    "                                bias_initializer=initializer, kernel_initializer=initializer)\n",
    "        norm = tf.contrib.layers.batch_norm(conv)\n",
    "        a = tf.nn.leaky_relu(norm)\n",
    "        pool = tf.layers.max_pooling2d(a, [2, 2], [2, 2], \"same\")\n",
    "        \n",
    "        conv = tf.layers.conv2d(pool, 128, [5, 5], padding=\"same\",\n",
    "                                bias_initializer=initializer, kernel_initializer=initializer)\n",
    "        norm = tf.contrib.layers.batch_norm(conv)\n",
    "        a = tf.nn.leaky_relu(norm)\n",
    "        pool = tf.layers.max_pooling2d(a, [2, 2], [2, 2], \"same\")\n",
    "        flatten = tf.layers.flatten(pool)\n",
    "        \n",
    "        dense = tf.layers.dense(flatten, 1, kernel_initializer=initializer, bias_initializer=initializer)\n",
    "        output = tf.sigmoid(dense)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build the model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "z_dim = 100\n",
    "\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "x_images = tf.placeholder(dtype=tf.float32, shape=[None, image_height, image_width, color_channels])\n",
    "z_noise = tf.placeholder(dtype=tf.float32, shape=[None, z_dim])\n",
    "\n",
    "Gz = create_generator(z_noise)\n",
    "Dx = create_discriminator(x_images)\n",
    "DGz = create_discriminator(Gz, reuse=True)\n",
    "\n",
    "g_loss = -tf.reduce_mean(tf.log(DGz))\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1 - DGz))\n",
    "\n",
    "g_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"generator\")\n",
    "d_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"discriminator\")\n",
    "\n",
    "g_train = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(g_loss, var_list=g_vars)\n",
    "d_train = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5).minimize(d_loss, var_list=d_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "load_checkpoint = False\n",
    "path = \"GAN checkpoints/\"\n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "model_name = \"cifar-gan\"\n",
    "\n",
    "batch_size = 200\n",
    "epochs = 15000\n",
    "display_step = 10\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "if load_checkpoint:\n",
    "    checkpoint = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "current_batch_index = 0\n",
    "\n",
    "test_z = np.random.uniform(-1.0, 1.0, size=[25, z_dim]).astype(np.float32)\n",
    "test_gen = create_generator(z_noise, reuse=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batch_xs = class_train_data[current_batch_index:current_batch_index + batch_size]\n",
    "    \n",
    "    if current_batch_index + batch_size >= len(class_train_data):\n",
    "        current_batch_index = 0\n",
    "    else:\n",
    "        current_batch_index += batch_size\n",
    "        \n",
    "    zs = np.random.uniform(-1.0, 1.0, size=[batch_size, z_dim]).astype(np.float32)\n",
    "        \n",
    "    if epoch % display_step == 0:\n",
    "        a = np.array(sess.run(test_gen, feed_dict={z_noise: test_z}))\n",
    "        save_sample(a, \"Generated/\" + str(epoch) + \".bmp\", [5, 5])\n",
    "        saver.save(sess, path + model_name, epoch)\n",
    "        \n",
    "    sess.run(d_train, feed_dict={x_images: batch_xs, z_noise: zs})\n",
    "    sess.run(g_train, feed_dict={z_noise: zs})\n",
    "    sess.run(g_train, feed_dict={z_noise: zs})\n",
    "    \n",
    "    print(\"Epoch\", \n",
    "          epoch, \n",
    "          \"Generator Loss\", \n",
    "          sess.run(g_loss, \n",
    "                   feed_dict={z_noise: zs}), \n",
    "          \"Discriminator Loss\", \n",
    "          sess.run(d_loss, \n",
    "                   feed_dict={x_images: batch_xs, z_noise: zs}))\n",
    "    \n",
    "saver.save(sess, path + model_name, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
